Machine learning lies at the intersection of statistics, computer science and optimisation.
The central idea is to design an algorithm that uses data to solve a problem, and in so avoid explicitly programming a solution.
With ever more data available and with ever more powerful computers, machine learning has become a powerful tool in many fields, solving problems previously thought intractable.
Such algorithms or models can be used for a plethora of tasks, which are mainly divided into three main categories:
\begin{description}

    \item[Supervised learning]
        Given data and labels, find the relationship and try to assign correct labels to new data.

    \item[Unsupervised learning]
        Given data, find some underlying structure, patterns, properties or relationships.

    \item[Reinforcement learning]
        Given a set of rules (such as a game), find a strategy to maximise some reward.

\end{description}
Only the first thereof will be explicitly considered in this thesis, though much of the theory and results can be extended to the latter two.
This chapter is hardly a comprehensive introduction to machine learning, but rather a brief overview of the most important concepts and techniques, serving as a reference point for the later endeavours into quantum machine learning.
Some familiarity with basic probability theory is assumed.

\section{Supervised learning}
Supervised learning is the most common and well-studied form of machine learning.
It has the benefit of easily being mathematically formulated, and it can apply statistical methods to solve the problem.
Given a data set
$
    \mathcal{D} = \{
    (\bm{x}^{(1)}, y^{(1)}), \
    \dots, \
    (\bm{x}^{(n)}, y^{(n)})
    \}
$
of $n$ samples, where $\bm{x}^{(i)}$ is a vector of features and $y^{(i)}$ is the corresponding label, the goal is to find a function $f$ that maps $\bm{x}$ to $y$.
In statistical terms, supervised learning can be interpreted as having samples from a joint distribution $p(\bm{x}, y)$ with the goal of finding a conditional distribution $p(y|\bm{x})$, or at least the expectation thereof.
The labels $y^{(i)}$ are usually assumed to be single-dimensional.
They may be categorical, in which case the problem is called classification, or continuous, in which case it is called regression.

The marginal distribution $p(y|\bm{x})$ if often thought of as decomposed into a known deterministic function $f(\bm{x})$ and a random noise term $\varepsilon$ such that the labels are given by
\begin{equation}
    y = f(\bm{x}) + \varepsilon
    \label{eq:ml_model}
\end{equation}
where $\varepsilon$ is assumed to be independent of $\bm{x}$.
This simplifies the problem to approximating the function $f(\bm{x})$.
Namely, find a function $\hat{f}$ such that the predictions $\hat{y}=\hat{f}(\bm{x})$ are good approximations of $y$.
The loss is a measure of how well the model fits the data.
In statistics, the (log-) likelihood is often used, while in machine learning, simpler, more naïve functions like mean square error (MSE) are often used.
% why?


\subsection{Parametric models}
Parametric models are a subclass of supervised learning models that are defined by a finite set of parameters $\bm{\theta}$.
This means that the model output is fully defined by the parameters, and the data is only used to estimate the parameters.

\subsection{Training}
Since the model is defined by the parameters, the loss function is a function of the parameters.
The supervised learning problem with a parametric model is rephrased into a standard optimisation problem:
\begin{equation}
    \bm{\theta}^* = \argmin_{\bm{\theta}} L(\bm{\theta; \mathcal{D}})
\end{equation}
where $L$ is the loss as a function of the parameters and given the data.
$\bm{\theta}^*$ is then the optimal set of parameters.
This optimisation is usually done using gradient descent methods, which means that the loss function needs to be differentiable with respect to the parameters.


\subsection{Bias-variance trade-off}
In machine learning, there is a constant struggle between having models with lots of parameters and great expressive power versus simpler models with fewer parameters.
The former are more likely to overfit the data, while the latter are more likely to underfit the data.
This is known as the bias-variance trade-off.
The main goal is of course to generalise, that is to have a model that truly captures the underlying properties of the data and subsequently performs well on data that it has not seen before.

Intrinsically, with an assumption like that of \Cref{eq:ml_model}, there is some uncertainty or noise that is inherent to the data.
Consequently, one must choose a model that is flexible enough to capture the underlying structure of the data, but not so flexible that it captures the noise.
When a model is too simple to capture the underlying structure, it is said to have high bias or be underfitted, while a model complex enough to capture the noise is said to have high variance or be overfitted.
An overfitted model will have small or no errors on the data used for training, but may be wildly inaccurate on new data.
This is captured in \Cref{fig:over_under_fit}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 1,
                        xlabels at=edge bottom,
                        ylabels at=edge left,
                        y descriptions at=edge left,
                        horizontal sep=0pt,
                    },
                width=0.4\textwidth,
                height=0.4\textwidth,
                xlabel={$x$},
                ylabel={$y$},
                ymin = -6, ymax = 6,
                grid=major,
                % tick label style={font=\footnotesize},
            ]
            \nextgroupplot[title={Degree 1}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_1, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 3}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_3, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 9}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_9, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};
        \end{groupplot}
    \end{tikzpicture}
    \caption{
        A simple example of overfitting and underfitting.
        Ten data points were generated by $x^3-2x$ plus some Gaussian noise with 0.4 standard deviation, shown in the figure as dots.
        The solid lines denote models fitted using least squares, being respectively polynomials of degree 1, 3 and 9, while the dashed line is the true function.
        The model with degree 1 is underfitted, while the model with degree 9 is overfitted — it perfectly fits all samples, but greatly deviates from the true function elsewhere.
        However, the \enquote{correct} cubic polynomial lies much closer to the true function.}
    \label{fig:over_under_fit}
\end{figure}