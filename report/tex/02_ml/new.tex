\section{Supervised learning}
For supervised learning, some data is assumed given.
Mathematically, this data is described by a set of input-output pairs $\mathcal{D} = \{(\bm{x}_i, y_i)\}_{i=1}^n$, where $\bm{x}_i\in\mathcal{X}$ is a vector of features in the input domain $\mathcal{X}$ and $y_i\in\mathcal{Y}$ is the corresponding label in the output domain $\mathcal{Y}$.
The data is assumed to be drawn from a joint distribution $p(\bm{x}, y)$.
Given a new, unseen input $\bm{x}$, the goal predict its corresponding label $y$.

The input domain $\mathcal{X}$ is usually assumed to be the vector space $\mathbb{R}^N$ for some integer $N$, though some dimensions may be discrete and even binary.
Some preprocessing is often done to the data, such rescaling the features or using the data to calculate new features.
While preprocessing can be essential to the performance of a machine learning model, it does not matter for this theoretical discussion; any preprocessed data can be assumed to be drawn from the distribution of the transformed variable.

The output domain $\mathcal{Y}$ is assumed to be single-dimensional, as a multidimensional prediction can be decomposed into multiple one-dimensional prediction problems.
It may be finite, in which case the problem is called classification, or continuous, in which case it is called regression.
Regression methods may easily be applied to classification problems trough simple thresholds.

\subsection{Models}
\subsubsection{Deterministic models}
In machine learning, models are typically deterministic and thus can be seen as functions $f:\mathcal{X}\to\mathcal{Y}$ that map inputs to outputs,
\begin{equation}
    \hat{y} = f(\bm{x}),
\end{equation}
where $\hat{y}$ is the predicted output.

A model belongs to some family, a set of models with similar properties, where families often are parametric.
Parametric models are defined by a finite set of parameters $\bm{\theta}= \{\theta_1, \ldots, \theta_k\}$, where each parameter $\theta_i$ is a real number.

\subsubsection{Probabilistic models}
With probabilistic models, the model instead attempts to reconstruct the conditional probability distribution $p(y|\bm{x})$.
In statistics, this distribution will often be of a known type which is parametrised by a function of the data and some parameters.
For instance, in linear regression, the distribution is assumed to be Gaussian with mean $\bm{\theta}^\top\bm{x}$ and variance given by another parameter $\sigma^2$.

Alternatively, the conditional distributions may be thought of as being defined by
\begin{equation}
    p(y|\bm{x}) = f(\bm{x}) + \varepsilon,
    \label{eq:probabilistic_model}
\end{equation}
where $\varepsilon$ is a random variable and $f$ a deterministic function.
The $\varepsilon$ can be seen as some inherent randomness or noise in the data, or as a result of lacking information.
It is often assumed to be independent of the input.

Probabilistic models can be converted to deterministic models using a method to extract a single output from the distribution, such as the maximum a posteriori,
\begin{equation}
    \hat{y} = \argmax_{y\in\mathcal{Y}} p(y|\bm{x}),
\end{equation}
or the mean,
\begin{equation}
    \hat{y} = \int_{\mathcal{Y}} y \, p(y|\bm{x})\mathrm{d}y.
\end{equation}

Probabilistic has the benefit of being able to provide a measure of uncertainty in the prediction, which is often useful.
With simpler settings, statistical arguments can be made about the relationship between the input and output, which can be used to make inferences about the data.
However, the simpler design of deterministic models makes them more suitable for computationally demanding tasks, such as deep learning.

\subsection{Measuring and optimising performance}
A loss function $L(y, \hat{y}): \mathcal{Y} \times \mathcal{Y} \to \mathbb{R}^+$ is used to measure the performance of a model by comparing a predicted output $\hat{y}$ to the true output $y$, which then is used to optimise the model.
By convention, the loss should be positive, and lower values indicate better predictions, zero indicating exact agreement.
Its main purpose is to optimise the model, so it need not be the most useful metric of performance.
It is usually chosen to be differentiable, as this allows for the use of gradient-based optimisation.
Therefore, something like $I(\hat y = y)$ is not used as a loss (though often as a performance metric).
Instead, losses like the square loss,
\begin{equation}
    L(y, \hat{y}) = (y - \hat{y})^2,
\end{equation}
often is used for regression problems and the cross-entropy,
\begin{equation}
    L(y, \hat{y}) = -y\log(\hat{y}) - (1-y)\log(1-\hat{y}),
\end{equation}
for classification problems.
If the model is probabilistic and assumed to be a particular type of distribution, its structure can be used to define a loss.
Maximum likelihood estimation can be used, switching only the sign to conform with machine learning conventions, leading to the negative log-likelihood loss.

With some loss, one can consider the risk, defined as the expected loss over the joint distribution of the data,
\begin{equation}
    R(f)
    = \text{E}(L(y, f(\bm{x})))
    = \iint_{\mathcal{X}\times\mathcal{Y}} L(y, f(\bm{x})) \, p(\bm{x}, y) \, \mathrm{d}\bm{x} \, \mathrm{d}y,
    \label{eq:risk}
\end{equation}
and wish to minimise it.
Of course, this is only possible if the joint distribution $p(\bm{x}, y)$ is known, so it is in practice replaced by the empirical risk, assuming independent and equally likely samples,
\begin{equation}
    \hat{R}(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\bm{x}_i)).
    \label{eq:empirical_risk}
\end{equation}
This can be assumed to converge to the true risk as $n\to\infty$ by the law of large numbers.
More data is certainly better, but it may be expensive to collect, and the deviance between the empirical and true risk may not decrease monotonically with $n$.
It is therefore desirable to find models and training methods that minimise the risk with as few samples as possible.


For instance, square loss leads to the mean squared error (MSE),
\begin{equation}
    \frac{1}{n} \sum_{i=1}^n (y_i - f(\bm{x}_i))^2.
\end{equation}
These empirical risks that depend on the whole data set defines cost functions that are used to train the model.

For a chosen parametric model family, the supervised learning problem can be formulated as an optimisation problem to find the best parameters $\bm{\theta}^*$:
\begin{equation}
    \bm{\theta}^* = \argmin_{\bm{\theta}} C(\bm{\theta; \mathcal{D}}),
\end{equation}
where $C$ is the cost function.
Since the loss function and models are often designed to be differentiable, giving differentiable costs, this can be solved using gradient descent methods.
In the simplest case, that means that the parameters are updated until some convergence criterion is met by
\begin{equation}
    \bm{\theta}^{(t+1)} = \bm{\theta}^{(t)} - \eta \nabla_{\bm{\theta}} C(\bm{\theta}^{(t)}, \mathcal{D}),
\end{equation}
where $\eta>0$ is a learning rate and $\bm{\theta}^{(t)}$ the parameters at iteration $t$.
In practice, more sophisticated gradient methods are often used, such as the Adam optimiser \cite{adamoptimiser}.

The process of optimising the cost function is called training.
As the model improves, it is said to learn.

\subsection{Generalisation: the bias-variance trade-off}
The goal of supervised learning is to find a model that performs well on unseen data.
Optimising the cost function on the training data will lead to a model that performs well on the training data, but may not generalise well to unseen data.
Therefore, the model is evaluated on a test set, which is not used for training.

There is a constant struggle between having models with lots of parameters and great expressive power versus simpler models with fewer parameters.
The former are more likely to overfit the data, while the latter are more likely to underfit the data.
This is known as the bias-variance trade-off.
The main goal is of course to generalise, that is to have a model that truly captures the underlying properties of the data and subsequently performs well on data that it has not seen before.

There may be mismatch between the true risk in \cref{eq:risk} and the empirical risk in \cref{eq:empirical_risk}.
Minimising the empirical risk may not be the same as minimising the true risk.
Intrinsically, with a probabilistic model, there is some uncertainty or noise that is inherent to the data.
Consequently, one must choose a model that is flexible enough to capture the underlying structure of the data, but not so flexible that it captures the noise.
When a model is too simple to capture the underlying structure, it is said to have high bias or be underfitted, while a model complex enough to capture the noise is said to have high variance or be overfitted.
An overfitted model will have small or no errors on the data used for training, but may be wildly inaccurate on new data.
This is captured in \cref{fig:over_under_fit}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 1,
                        xlabels at=edge bottom,
                        ylabels at=edge left,
                        y descriptions at=edge left,
                        horizontal sep=0pt,
                    },
                width=0.4\textwidth,
                height=0.4\textwidth,
                xlabel={$x$},
                ylabel={$y$},
                ymin = -6, ymax = 6,
                grid=major,
                % tick label style={font=\footnotesize},
            ]
            \nextgroupplot[title={Degree 1}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_1, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 3}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_3, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 9}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_9, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};
        \end{groupplot}
    \end{tikzpicture}
    \caption{
        A simple example of overfitting and underfitting.
        Ten data points were generated by $x^3-2x$ plus some Gaussian noise with 0.4 standard deviation, shown in the figure as dots.
        The solid lines denote models fitted using squared loss, being respectively polynomials of degree 1, 3 and 9, while the dashed line is the true function.
        The model with degree 1 is underfitted, while the model with degree 9 is overfitted — it perfectly fits all samples, but greatly deviates from the true function elsewhere.
        However, the \enquote{correct} cubic polynomial lies much closer to the true function.}
    \label{fig:over_under_fit}
\end{figure}