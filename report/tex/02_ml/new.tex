\section{Supervised learning}
\label{sec:supervised_learning}

\subsection{Data}
For supervised learning, some data is assumed given.
Mathematically, this data is described by a set of input-output pairs $\mathcal{D} = \{(\bm{x}^{(i)}, y^{(i)})\}_{i=1}^n$, where $\bm{x}^{(i)} \in\mathcal{X}$ is a vector of features in the input domain $\mathcal{X}$ and $y^{(i)}\in\mathcal{Y}$ is the corresponding label in the output domain $\mathcal{Y}$.
The data is assumed to be drawn from a joint distribution $p(\bm{x}, y)$.
Given a new, unseen input $\bm{x}$, the goal predict its corresponding label $y$.

The input domain $\mathcal{X}$ is usually assumed to be the vector space $\mathbb{R}^N$ for some integer $N$, though some dimensions may be discrete and even binary.
Some preprocessing is often done to the data, such rescaling the features or using the data to calculate new features.
While preprocessing can be essential to the performance of a machine learning model, it does not matter for this theoretical discussion; any preprocessed data can be assumed to be drawn from the distribution of the transformed variable.

The output domain $\mathcal{Y}$ is assumed to be single-dimensional, as a multidimensional prediction can be decomposed into multiple one-dimensional prediction problems.
It may be finite, in which case the problem is called classification, or continuous, in which case it is called regression.
Regression methods may easily be applied to classification problems through simple thresholds.

\subsubsection{Example: Fisher's \textit{Iris} data set}
\label{sec:iris}
To have a concrete example, consider Fisher's \textit{Iris} data set \cite{fisher1936}, containing samples of three different species of the flowering plant in the genus \textit{Iris}.
There are 50 samples of each species, giving a total of 150 samples, and each sample has four features: sepal length, sepal width, petal length and petal width.
\Cref{tab:iris} lists some samples.
The goal is to predict the species of a new sample given its features, where the species is usually encoded ordinally as
\begin{equation}
    \label{eq:iris_encoding}
    \begin{split}
        \textit{Setosa} &\mapsto 0,\\
        \textit{Versicolor} &\mapsto 1,\\
        \textit{Virginica} &\mapsto 2,
    \end{split}
\end{equation}
or as a one-hot vector by
\begin{equation}
    \label{eq:iris_one_hot}
    \begin{split}
        \textit{Setosa} &\mapsto (1, 0, 0)^\top,\\
        \textit{Versicolor} &\mapsto (0, 1, 0)^\top,\\
        \textit{Virginica} &\mapsto (0, 0, 1)^\top.
    \end{split}
\end{equation}
One-hot encoding is better suited for multi-class classification problems like this, allowing probabilistic predictions of the respective classes, but for binary classification problems, ordinal encoding is more convenient, needing only a single to describe both probabilities.
In Fisher's original paper, linear discriminant analysis was used to classify the flowers.
Since then, the data set has become a standard benchmark for statistical classification techniques and machine learning methods like support vector machines.


\begin{table}
    \centering
    \caption{
        Fisher's \textit{Iris} data set.
        The premier column, species, is the class label.
        The latter four columns are the features, all measured in centimetres.
    }
    \label{tab:iris}
    \begin{tblr}{
            width=\linewidth,
            colspec={Q[l,1] Q[r,2] Q[r,2] Q[r,2] Q[r,2]},
        }
        \toprule
        \textbf{Species}    & \textbf{Sepal length} & \textbf{Sepal width} & \textbf{Petal length} & \textbf{Petal width} \\
        \midrule
        \textit{Setosa}     & 5.1 cm                & 3.5 cm               & 1.4 cm                & 0.2 cm               \\
        \textit{Setosa}     & 4.9 cm                & 3.0 cm               & 1.4 cm                & 0.2 cm               \\
        \dots               & \dots                 & \dots                & \dots                 & \dots                \\
        \textit{Versicolor} & 7.0 cm                & 3.2 cm               & 4.7 cm                & 1.4 cm               \\
        \dots               & \dots                 & \dots                & \dots                 & \dots                \\
        \textit{Virginica}  & 6.3 cm                & 3.3 cm               & 6.0 cm                & 2.5 cm               \\
        \dots               & \dots                 & \dots                & \dots                 & \dots                \\
        \bottomrule
    \end{tblr}
\end{table}


\subsection{Models}
\subsubsection{Deterministic models}
In machine learning, models are typically deterministic and thus can be seen as functions $f:\mathcal{X}\to\mathcal{Y}$ that map inputs to outputs,
\begin{equation}
    \hat{y} = f(\bm{x}),
\end{equation}
where $\hat{y}$ is the predicted output.
For instance, a deterministic model for Fisher's \textit{Iris} data set would take in some $\bm{x}=(x_1,x_2,x_3,x_4)\in\mathbb{R}^4$ and output a species $\hat y\in\{0,1,2\}$ as per \cref{eq:iris_encoding}.

A model belongs to some family, a set of models with similar properties, where families often are parametric.
Parametric models are defined by a finite set of parameters $\bm{\theta}= \{\theta_1, \ldots, \theta_k\}$, where each parameter $\theta_i$ is a real number.
Linear regression, for example, is a parametric model with parameters $\bm{\theta} = \{w_1, \ldots, w_N, b\}$, defining a linear function $f(\bm{x}) = \bm{w}^\top\bm{x} + b$.

\subsubsection{Probabilistic models}
With probabilistic models, the model instead attempts to reconstruct the conditional probability distribution $p(y|\bm{x})$.
In statistics, this distribution will often be of a known type which is parametrised by a function of the data and some parameters.
For example, a linear regression model can be used as a probabilistic model with a Gaussian distribution for the output, such that the conditional distribution is given by $p(y|\bm{x}) = \mathcal{N}(\bm{w}^\top\bm{x} + b, \sigma^2)$, where $\bm{w}$, $b$ and $\sigma$ are the parameters of the model. A probabilistic model for Fisher's \textit{Iris} data set would take in some $\bm{x}=(x_1,x_2,x_3,x_4)\in\mathbb{R}^4$ and output a probability vector $p(y|\bm{x})\in [0,1]^3$.
Classification models are often probabilistic, predicting the probability of each class.
In that way, the model output is continuous and potentially differentiable.

Alternatively, the conditional distributions may be thought of as being defined by
\begin{equation}
    p(y|\bm{x}) = f(\bm{x}) + \varepsilon,
    \label{eq:probabilistic_model}
\end{equation}
where $\varepsilon$ is a random variable and $f$ a deterministic function.
The $\varepsilon$ can be seen as some inherent randomness or noise in the data, or as a result of lacking information.
It is often assumed to be independent of the input.

Probabilistic models can be converted to deterministic models using a method to extract a single output from the distribution, such as the maximum a posteriori,
\begin{equation}
    \hat{y} = \argmax_{y\in\mathcal{Y}} p(y|\bm{x}),
\end{equation}
or the mean,
\begin{equation}
    \hat{y} = \int_{\mathcal{Y}} y \, p(y|\bm{x})\mathrm{d}y,
\end{equation}
for regression problems.

Probabilistic models have the benefit of being able to provide a measure of uncertainty in the prediction, which is often useful.
With simpler settings, statistical arguments can be made about the relationship between the input and output, which can be used to make inferences about the data.
However, the simpler design of deterministic models makes them more suitable for computationally demanding tasks, such as deep learning, and in automated settings, where decisions need to be made quickly and uncertainty is not a concern.

\subsection{Measuring and optimising performance}
A loss function, $L(y', \hat{y})$ for deterministic models or $L(y', p(y|\bm{x}))$ for probabilistic models, is used to optimise a model by comparing its predictions to the true labels $y'$.
By convention, the loss should be positive, and lower values indicate better predictions, zero indicating exact agreement.
Its main purpose is to optimise the model, so it need not be the most useful metric of performance.
It is usually chosen to be differentiable, as this allows for the use of gradient-based optimisation.
Therefore, something like $I(\hat y = y')$ is not used as a loss (though often as a performance metric).
Instead, losses like the square loss,
\begin{equation}
    L(y', \hat{y}) = (y' - \hat{y})^2,
\end{equation}
is often used for regression problems.

If the model is probabilistic and assumed to be a particular type of distribution, its structure can be used to define a loss.
Maximum likelihood estimation can be used, switching only the sign to conform with machine learning conventions, leading to the negative log-likelihood (NLL) loss,
\begin{equation}
    L(y', p(y|\bm{x})) = -\log p(y'|\bm{x}).
\end{equation}
This loss is often used for classification problems, where the output is a probability vector $(p_0, p_1, \dots, p_N)^\top$, and the true label is encoded as a one-hot vector $y'=(0, 0, \dots, 1, \dots, 0)^\top$.
It then reads
\begin{equation}
    L(y', p(y|\bm{x})) = \sum_{i=1}^k y'_i \log p_i,
\end{equation}
where $k$ is the number of classes.
This formulation is referred to as cross-entropy loss in machine learning.

With some loss, one can consider the risk, defined as the expected loss over the joint distribution of the data,
\begin{equation}
    R(f)
    = \text{E}(L(y', f(\bm{x})))
    = \iint_{\mathcal{X}\times\mathcal{Y}} L(y', f(\bm{x})) \, p(\bm{x}, y) \, \mathrm{d}\bm{x} \, \mathrm{d}y,
    \label{eq:risk}
\end{equation}
and wish to minimise it.
Of course, this is only possible if the joint distribution $p(\bm{x}, y)$ is known, so it is in practice replaced by the empirical risk, assuming independent and equally likely samples,
\begin{equation}
    \label{eq:empirical_risk}
    \hat{R}(f) = \frac{1}{n} \sum_{i=1}^n L(y^{(i)}, f(\bm{x}^{(i)})).
\end{equation}
This can be assumed to converge to the true risk as $n\to\infty$ by the law of large numbers.
More data is certainly better, but it may be expensive to collect, and the deviance between the empirical and true risk may not decrease monotonically with $n$.
It is therefore desirable to find models and training methods that can minimise the true risk with as few samples as possible.

These empirical risks that depend on the whole data set defines cost functions that are used to train the model.
For instance, square loss leads to the mean squared error (MSE),
\begin{equation}
    C(f;\mathcal{D}) = \frac{1}{n} \sum_{i=1}^n (y'^{(i)} - f(\bm{x}^{(i)}))^2,
\end{equation}
while a likelihood based loss leads to
\begin{equation}
    C(p;\mathcal{D}) = -\frac{1}{n} \sum_{i=1}^n \log p(y'^{(i)}|\bm{x}^{(i)}).
\end{equation}

For a chosen parametric model family, the supervised learning problem can be formulated as an optimisation problem to find the best parameters $\bm{\theta}^*$:
\begin{equation}
    \bm{\theta}^* = \argmin_{\bm{\theta}} C(\bm{\theta; \mathcal{D}}),
\end{equation}
where the cost function is parametrised by the model parameters $\bm{\theta}$.
Since the loss function and models are often designed to be differentiable, giving differentiable costs, this can be solved using gradient descent methods.
In the simplest case, that means that the parameters are updated until some convergence criterion is met by
\begin{equation}
    \bm{\theta}^{(t+1)} = \bm{\theta}^{(t)} - \eta \nabla_{\bm{\theta}} C(\bm{\theta}^{(t)}, \mathcal{D}),
\end{equation}
where $\eta>0$ is a learning rate and $\bm{\theta}^{(t)}$ the parameters at iteration $t$.
In practice, more sophisticated gradient methods are often used, such as the Adam optimiser \cite{adamoptimiser}.

The process of optimising the cost function is called training.
As the model improves, it is said to learn.

\subsection{Generalisation: the bias-variance trade-off}
The goal of supervised learning is to find a model that performs well on unseen data.
Optimising the cost function on the training data will lead to a model that performs well on the training data, but may not generalise well to unseen data.
Therefore, the model is evaluated on a test set, which is not used for training.

There is a constant struggle between having models with lots of parameters and great expressive power versus simpler models with fewer parameters.
The former are more likely to overfit the data, while the latter are more likely to underfit the data.
This is known as the bias-variance trade-off.
The main goal is of course to generalise, that is to have a model that truly captures the underlying properties of the data and subsequently performs well on data that it has not seen before.

There may be mismatch between the true risk in \cref{eq:risk} and the empirical risk in \cref{eq:empirical_risk}.
Minimising the empirical risk may not be the same as minimising the true risk.
Generally, there is some uncertainty or noise that is inherent to the data.
Consequently, one must choose a model that is flexible enough to capture the underlying structure of the data, but not so flexible that it captures the noise.
When a model is too simple to capture the underlying structure, it is said to have high bias or be underfitted, while a model complex enough to capture the noise is said to have high variance or be overfitted.
An overfitted model will have small or no errors on the data used for training, but may be wildly inaccurate on new data.
This is captured in \cref{fig:over_under_fit}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 1,
                        xlabels at=edge bottom,
                        ylabels at=edge left,
                        y descriptions at=edge left,
                        horizontal sep=0pt,
                    },
                width=0.4\textwidth,
                height=0.4\textwidth,
                xlabel={$x$},
                ylabel={$y$},
                ymin = -6, ymax = 6,
                grid=major,
                % tick label style={font=\footnotesize},
            ]
            \nextgroupplot[title={Degree 1}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_1, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 3}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_3, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 9}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_9, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};
        \end{groupplot}
    \end{tikzpicture}
    \caption{
        A simple example of overfitting and underfitting.
        Ten data points were generated by $x^3-2x$ plus some Gaussian noise with 0.4 standard deviation, shown in the figure as dots.
        The solid lines denote models fitted using squared loss, being respectively polynomials of degree 1, 3 and 9, while the dashed line is the true function.
        The model with degree 1 is underfitted, while the model with degree 9 is overfitted — it perfectly fits all samples, but greatly deviates from the true function elsewhere.
        However, the \enquote{correct} cubic polynomial lies much closer to the true function.}
    \label{fig:over_under_fit}
\end{figure}