\section{Supervised learning}
For supervised learning, some data is assumed given.
Mathematically, this data is described by a set of input-output pairs $\mathcal{D} = \{(\bm{x}_i, y_i)\}_{i=1}^n$, where $\bm{x}_i\in\mathcal{X}$ is a vector of features in the input domain $\mathcal{X}$ and $y_i\in\mathcal{Y}$ is the corresponding label in the output domain $\mathcal{Y}$.
The data is assumed to be drawn from a joint distribution $p(\bm{x}, y)$.
Given a new input $\bm{x}$, the goal predict the corresponding label $y$.

The input domain $\mathcal{X}$ is usually assumed to be the vector space $\mathbb{R}^N$ for some integer $N$, though some dimensions may be discrete and even binary.
Some preprocessing is often done to the data, such rescaling the features or using the data to calculate new features.
While preprocessing can be essential to the performance of a machine learning model, it does not matter for this theoretical discussion; any preprocessed data can be assumed to be drawn from a corresponding joint distribution.

The output domain $\mathcal{Y}$ is assumed to be single-dimensional, as a multidimensional prediction can be decomposed into multiple one-dimensional prediction problems.
It may be finite, in which case the problem is called classification, or continuous, in which case it is called regression.
Regression methods may easily be applied to classification problems trough simple thresholds.

\subsection{Models}
\subsubsection{Deterministic models}
In machine learning, models are typically deterministic and thus can be seen as functions $f:\mathcal{X}\to\mathcal{Y}$ that map inputs to outputs,
\begin{equation}
    \hat{y} = f(\bm{x}),
\end{equation}
where $\hat{y}$ is the predicted output.

A model belongs to some family, a set of models with similar properties, where families often are parametric.
Parametric models are defined by a finite set of parameters $\bm{\theta}= \{\theta_1, \ldots, \theta_n\}$, where each parameter $\theta_i$ is a real number.

\subsubsection{Probabilistic models}
With probabilistic models, a conditional probability distribution $p(y|\bm{x})$ is used instead of a deterministic function.
Alternatively, the distribution may be thought of as being defined by
\begin{equation}
    p(y|\bm{x}) = f(\bm{x}) + \epsilon,
    \label{eq:probabilistic_model}
\end{equation}
where $\epsilon$ is a random variable and $f$ a deterministic function.
The $\epsilon$ can be seen as some inherent randomness or noise in the data, or as a result of lacking information.
It is often assumed to be independent of the input.
These models are also often assumed to be parametrised by some parameters $\bm{\theta}$.

Probabilistic models can be converted to deterministic models using a method to extract a single output from the distribution, such as the maximum a posterior
\begin{equation}
    \hat{y} = \argmax_{y\in\mathcal{Y}} p(y|\bm{x})
\end{equation}
or the mean
\begin{equation}
    \hat{y} = \int_{\mathcal{Y}} y \, p(y|\bm{x})\mathrm{d}y.
\end{equation}


\subsection{Measuring and optimising performance}
The loss function $L(y, \hat{y})$ is used to measure the performance of a model by comparing a predicted output $\hat{y}$ to the true output $y$.
It is usually chosen to be differentiable, as this allows for the use of gradient descent methods.
Therefore, losses like the square loss,
\begin{equation}
    L(y, \hat{y}) = \frac{1}{n} (y - \hat{y})^2,
\end{equation}
is often used for regression problems and the cross-entropy,
\begin{equation}
    L(y, \hat{y}) = -y\log(\hat{y}) - (1-y)\log(1-\hat{y}),
\end{equation}
for classification problems.

With some loss, one can consider the risk, defined as
\begin{equation}
    R(f) = \iint_{\mathcal{X}\times\mathcal{Y}} L(y, f(\bm{x})) \, p(\bm{x}, y) \, \mathrm{d}\bm{x} \, \mathrm{d}y,
    \label{eq:risk}
\end{equation}
and wish to minimise it.
Of course, this is only possible if the joint distribution $p(\bm{x}, y)$ is known, so it is in practice replaced by the empirical risk,
\begin{equation}
    R(f) = \frac{1}{n} \sum_{i=1}^n L(y_i, f(\bm{x}_i)).
    \label{eq:empirical_risk}
\end{equation}
For instance, square loss leads to the mean squared error (MSE),
\begin{equation}
    \frac{1}{n} \sum_{i=1}^n (y_i - f(\bm{x}_i))^2.
\end{equation}
These empirical risks that depend on the whole data set defines cost functions that are used to train the model.

For a chosen parametric model family, the supervised learning problem can be formulated as an optimisation problem:
\begin{equation}
    \bm{\theta}^* = \argmin_{\bm{\theta}} C(\bm{\theta; \mathcal{D}}),
\end{equation}
where $C$ is the cost function.
The loss function and models are often chosen to be differentiable, giving differentiable costs, which allows for the use of gradient descent methods.
In the simplest case, that means that the parameters are updated until some convergence criteria is met by
\begin{equation}
    \bm{\theta}^{(t+1)} = \bm{\theta}^{(t)} - \eta \nabla_{\bm{\theta}} C(\bm{\theta}^{(t)}, \mathcal{D}),
\end{equation}
where $\eta>0$ is a learning rate and $\bm{\theta}^{(t)}$ is the parameter vector at iteration $t$.
In practice, more sophisticated methods are often used, such as the Adam optimiser \cite{adamoptimiser}.

The process of optimising the cost function is called training.
As the model improves, it is said to learn.

\subsection{Bias-variance trade-off}
In machine learning, there is a constant struggle between having models with lots of parameters and great expressive power versus simpler models with fewer parameters.
The former are more likely to overfit the data, while the latter are more likely to underfit the data.
This is known as the bias-variance trade-off.
The main goal is of course to generalise, that is to have a model that truly captures the underlying properties of the data and subsequently performs well on data that it has not seen before.

There may be mismatch between the true risk in \cref{eq:risk} and the empirical risk in \cref{eq:empirical_risk}.
Minimising the empirical risk may not be the same as minimising the true risk.
Intrinsically, with a probabilistic model, there is some uncertainty or noise that is inherent to the data.
Consequently, one must choose a model that is flexible enough to capture the underlying structure of the data, but not so flexible that it captures the noise.
When a model is too simple to capture the underlying structure, it is said to have high bias or be underfitted, while a model complex enough to capture the noise is said to have high variance or be overfitted.
An overfitted model will have small or no errors on the data used for training, but may be wildly inaccurate on new data.
This is captured in \cref{fig:over_under_fit}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 1,
                        xlabels at=edge bottom,
                        ylabels at=edge left,
                        y descriptions at=edge left,
                        horizontal sep=0pt,
                    },
                width=0.4\textwidth,
                height=0.4\textwidth,
                xlabel={$x$},
                ylabel={$y$},
                ymin = -6, ymax = 6,
                grid=major,
                % tick label style={font=\footnotesize},
            ]
            \nextgroupplot[title={Degree 1}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_1, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 3}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_3, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 9}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_9, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};
        \end{groupplot}
    \end{tikzpicture}
    \caption{
        A simple example of overfitting and underfitting.
        Ten data points were generated by $x^3-2x$ plus some Gaussian noise with 0.4 standard deviation, shown in the figure as dots.
        The solid lines denote models fitted using least squares, being respectively polynomials of degree 1, 3 and 9, while the dashed line is the true function.
        The model with degree 1 is underfitted, while the model with degree 9 is overfitted â€” it perfectly fits all samples, but greatly deviates from the true function elsewhere.
        However, the \enquote{correct} cubic polynomial lies much closer to the true function.}
    \label{fig:over_under_fit}
\end{figure}