\chapter{Machine learning}
\label{chap:ml}
Machine learning lies at the intersection of statistics, computer science and optimisation.
The central idea is to design an algorithm that uses data to solve a problem, and in so avoid explicitly programming a solution.
Such algorithms or models can be used for a plethora of tasks, which is mainly divided into three major categories:
\begin{itemize}
    \item \textbf{Supervised learning}: Given data and labels, find the relationship and try to be able to assign correct labels to new data.
    \item \textbf{Unsupervised learning}: Given data, find some underlying structure, patterns, properties or relationships.
    \item \textbf{Reinforcement learning}: Given some rules (e.g. a game), find a strategy to maximise some reward.
\end{itemize}
Only the first thereof will be explicitly considered in this thesis, though much of the theory and results can be extended to the latter two.

\section{Supervised learning}
Supervised learning is the most common and well-studied form of machine learning.
It has the benefit of easily being mathematically formulated, and it can apply statistical methods to solve the problem.
Given a data set
$
    \mathcal{D} = \{
    (\bm{x}^{(1)}, y^{(1)}), \
    \dots, \
    (\bm{x}^{(n)}, y^{(n)})
    \}
$
of $n$ samples, where $\bm{x}^{(i)}$ is a vector of features and $y^{(i)}$ is the corresponding label, the goal is to find a function $f$ that maps $\bm{x}$ to $y$.
In statistical terms, supervised learning can be thought of as having samples from a joint distribution $p(\bm{x}, y)$ with the goal of to find a conditional distribution $p(y|\bm{x})$, or at least the expectation thereof.
The labels $y^{(i)}$ are usually assumed to be single-dimensional.
They may be categorical, in which case the problem is called classification, or continuous, in which case it is called regression.

The marginal distribution $p(y|\bm{x})$ if often thought of as decomposed into a known deterministic function $f(\bm{x})$ and a random noise term $\varepsilon$ such that the labels are given by
\begin{equation}
    y = f(\bm{x}) + \varepsilon
    \label{eq:ml_model}
\end{equation}
where $\varepsilon$ is assumed to be independent of $\bm{x}$. This simplifies the problem to approximating the function $f(\bm{x})$.

\subsection{Parametric models}
Parametric models are a subclass of supervised learning models that are defined by a finite set of parameters $\bm{\theta}$.
This means that the model is fully defined by the parameters, and the data is only used to estimate the parameters.

\subsection{Training}
The parameters $\bm{\theta}$ are usually estimated by optimising some loss function $L$.
The loss is a measure of how well the model fits the data. In statistics, the (log-) likelihood is often used, while in machine learning, simpler, more na√Øve functions like mean square error (MSE) are often used.
The loss function is function of the parameters $\bm{\theta}$, but depends on the data.

With a loss function defined, the machine learning problem is rephrased into a standard optimisation problem:
\begin{equation}
    \bm{\theta}^* = \argmin_{\bm{\theta}} L(\bm{\theta; \mathcal{D}})
\end{equation}
where $\bm{\theta}^*$ is the optimal set of parameters.
This is usually done using gradient descent methods, which means that the loss function should be differentiable with respect to the parameters.



\section{Bias-variance trade-off}
In machine learning, there is a constant struggle between having models with lots of parameters and great expressive power versus simpler models with fewer parameters.
The former are more likely to overfit the data, while the latter are more likely to underfit the data.
This is known as the bias-variance trade-off.
The main goal is of course to \emph{generalise}, that is to have a model that truly captures the underlying properties of the data and subsequently performs well on data that it has not seen before.

Intrinsically, with a model like that of \cref{eq:ml_model}, there is some noise that is not captured by the model.
Consequently, one must choose a model that is flexible enough to capture the underlying structure of the data, but not so flexible that it captures the noise.
When a model is too simple to capture the underlying structure, it is said to have high bias or be underfitted, while a model complex enough to capture the noise is said to have high variance or be overfitted.
An overfitted model will have a low or zero errors on the data used for training, but may be wildly inaccurate on new data.
This is captured in \cref{fig:over_under_fit}.

\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[
                group style={
                        group size=3 by 1,
                        xlabels at=edge bottom,
                        ylabels at=edge left,
                        y descriptions at=edge left,
                        horizontal sep=0pt,
                    },
                width=0.4\textwidth,
                height=0.4\textwidth,
                xlabel={$x$},
                ylabel={$y$},
                ymin = -5, ymax = 5,
                grid=major,
                % tick label style={font=\footnotesize},
            ]
            \nextgroupplot[title={Degree 1}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_1, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 3}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_3, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};

            \nextgroupplot[title={Degree 9}]
            \addplot[only marks]  table[x=x, y=y, col sep=comma] {../code/over_under_fit/samples.csv};
            \addplot[mark=none]  table[x=x, y=deg_9, col sep=comma] {../code/over_under_fit/preds.csv};
            \addplot[mark=none, dashed]  table[x=x, y=true, col sep=comma] {../code/over_under_fit/preds.csv};
        \end{groupplot}
    \end{tikzpicture}
    \caption{
        A simple example of overfitting and underfitting.
        10 data points were generated by $x^3-2x$ plus some Gaussian noise with 0.4 standard deviation, shown in the figure as dots.
        The solid lines denote the fitted models which are polynomials of degree 1, 3 and 9, while the dashed line is the true function.
        The model with degree 1 is underfitted, while the model with degree 9 is overfitted.
        However, the \enquote{correct} cubic polynomial lies much closer to the true function.}
    \label{fig:over_under_fit}
\end{figure}

% More paramaters generally more variance, overfitting
% Too few, not expressive enough, underfitting
% How to find the right balance? hard

% There are methods like regularisation, cross-validation, early stopping, dropout, etc.
% Not ideal but kind of works

% The main goal is generalisation, actually getting to the underlying distribution

% Double descent, deep learning, brute force? NN and modern ml

\section{Neural networks}

\subsection{Convolutional neural networks}

\subsection{Backpropagation}
\subsection{Universal approximation theorem}