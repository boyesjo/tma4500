\chapter{Introduction}
\label{chap:intro}
Quantum computing offers a new paradigm for computation.
Using quantum properties such as superposition and entanglement, quantum computers can solve problems that are intractable for classical computers.
Their first conception goes back to the early 80s, often being accredited to Richard Feynman's 1982 paper \cite{feynman1982}, with the goal of simulating difficult quantum mechanical problems.
It was however only really with the discovery of Shor's algorithm in 1994 \cite{shor1994} that the potential of quantum computers gained widespread attention.
Shor's is a quantum algorithm that can factor large numbers in polynomial time, a problem that is believed to be practically exponentially hard and therefore intractable for classical computers.
Since then, the search has continued for what other tasks can be solved more efficiently on quantum hardware.

How actually to construct a quantum computer is still an open question, and perhaps a more pressing one.
There are several types of hardware being developed and researched, such as superconducting circuits used by IBM, Google and more, trapped ions used by IonQ and Honeywell, photonic quantum computers developed by Xanadu and Psi Quantum in addition to many other types.
Although some of these have already claimed quantum supremacy, that is they have solved a problem believed to be intractable for classical computers, supremacy has only been shown in very particular settings with no practical use.
Common for all current approaches are difficulties with noise and decoherence.
Theoretically, with computers of great enough scale, errors can be mitigated, but in the near future, the systematic errors of quantum computers will be a limiting factor and something to be taken into account when designing algorithms.
Hence, the focus of much current quantum computing research considers noisy intermediate scale quantum (NISQ) devices.
How to overcome the difficulties of NISQ hardware and be able to extract the full potential of quantum computers is a current research topic.

Variational quantum algorithms (VQAs) have been seen as a promising approach to use NISQ devices to solve problems with actual practical use.
Such algorithms use a classical optimiser to find the best parameters for a general algorithm.
In this way, the quantum hardware is only used for a small part of the algorithm, and the rest is done classically.
Consequently, there is less time for noise and decoherence to compound, which would ruin the computation, and the efficiency of classical hardware can still be utilised.

VQAs are in a sense a very natural approach, as most quantum hardware are inherently parametrised; microwave pulses for superconducting circuits, laser pulses for trapped ions and so on must use a particular pulse length, frequency et cetera.
Instead of calibrating the hardware to follow a Platonic algorithm, it could instead be better to calibrate it to map the input to the desired output.

Applications of VQAs are numerous.
A typical example is finding the ground state of a Hamiltonian for a molecule.
Such problems are exponential in the particle count, and thus intractable on classical hardware for larger molecules, while the process of evaluating the Hamiltonian on quantum hardware is typically polynomial.
VQAs are also well suited for general mathematical tasks such as optimisation.

Machine learning (ML) and VQAs are a good fit, as the optimisation of parameters is how many machine learning models are trained.
With some way of encoding data into quantum hardware, VQAs are easily interpreted as machine learning models.
The output of the quantum algorithm can be seen as a prediction in supervised learning, and so the quantum model can be trained to predict.
Though less efficient than classical backpropagation, gradients can be calculated for VQAs, and thus optimisation can be accomplished using classical optimisers.
With gate-based quantum computers, the quantum model can show some similarities to classical neural networks, bringing forth the notion of quantum neural nets (QNNs).
QNNs are a promising approach to machine learning on quantum hardware, for which ideas from classical neural networks can be applied, but the lack of backpropagation and potentially vanishing gradients are challenges.
% Moreover, the idea of encoding data into to high-dimensional quantum state is reminiscent of classical kernel methods.
Research indicate some advantages of using quantum machine learning models over classical ones, such as requiring fewer training iterations, but the field of quantum machine learning (QML) is still in its infancy, and much work remains to be done.
In particular, whether any quantum advantage for ML can be achieved with NISQ devices is not certain, and no exponential speed-up has yet been demonstrated.

The study of quantum machine learning is not an easy one.
A lot of machine learning's success is anecdotal; it is often difficult to prove correctness and convergence, so some trust of ML relies on empirical testing.
Obviously, quantum computing does not simplify matters.
Being restrained to current day hardware, there is little chance that any advantage for quantum hardware can be shown empirically, and so arguments for quantum advantage must be made theoretically or by extrapolation of simple tests.
Most research thus far can only be seen as proof of concepts, showing some benefits on trivial problems.

This thesis serves as an introduction to the field of quantum machine learning and quantum neural networks.
The two next chapters give a theoretical background.
\Cref{chap:ml} reviews the basics of machine learning and neural networks in particular.
Thereafter, \cref{chap:qc} introduces the basics of quantum computing and the limitations of NISQ devices in addition to an introduction to variational quantum algorithms.
In \cref{chap:qml}, how data is encoded into quantum hardware is first discussed. Next, quantum neural networks are introduced.
The penultimate \cref{chap:comparison} presents various simulations of quantum classical neural networks and a comparison with classical neural networks.
Finally, \cref{chap:conclusion} concludes the thesis and discusses future work.
% \section{Relevant work}


% \subsection{Cerezo et al. (2021)}
% Variational quantum algorithms (VQAs) are envisioned as the most likely candidate for quantum advantage to be achieved. By optimising a set of parameters that describe the quantum circuit, classical optimisation techniques are applicable, and only using the quantum hardware for what can be interpreted as function calls, limits the circuit depths needed. Running the same circuit many times with slightly different parameters and inputs in a classical-quantum-hybrid fashion, rather than a complete quantum implementation, means that the quantum operations can be simple enough for the noise and decoherence to be manageable.

% Generally, VQAs start with defining a cost function, depending on some input data (states) and the parametrised circuit, to be minimised with respect to the parameters of the quantum circuit. For example, the cost function for the variational quantum eigensolver (VQE) is the expectation value of some Hamiltonian, which is the energy of a system. The cost function should be meaningful in the sense that the minimum coincides with the optimal solution to the problem, and that lower values generally implies better solutions. Additionally, the cost function should be complicated enough to warrant quantum computation by not being easily calculated on classical hardware, while still having few enough parameters to be efficiently optimised.

% The optimisation of the cost function is often done with gradient descent methods. To evaluate the gradient of the quantum circuit w.r.t. the parameters, the very convenient parameter shift rule is often used. Though appearing almost as a finite difference scheme, relying on evaluating the circuit with slightly shifted parameters, it is indeed and exact formula. Furthermore, it may be used recursively to evaluate higher order derivatives, which is useful for optimisation methods that require the Hessian.

% VQA's applications are numerous. The archetypical example is finding the ground state of a Hamiltonian for a molecule. Such problems are exponential in the particle count, and thus intractable on classical hardware for larger molecules, while the problem of evaluating the Hamiltonian on quantum hardware is typically polynomial. VQAs are also well suited for general mathematical problems and optimisation, even machine learning, another common example being QAOA for the max-cut problem.


% Still, there are many difficulties when applying VQAs. Barren plateaus are a common occurrence, making the optimisation futile. The choosing of the ansatz determines the performance and feasibility of the algorithms, and there are many strategies and options. Some rely on exploiting the specific quantum hardware's properties, while some use the specifics of the problem at hand. Finally, the inherent noise and errors on near-term hardware will still be a problem and limit circuit depths.


% \subsection{Moll et al. (2018)}
% The computational performance of quantum computers is decided by five main factors. Naturally, the total qubit count is important, but also their connectivity (if they are not connected, intermediate operations like swapping is needed). How many gates/operations can be used before decoherence, noise and errors ruins the result also determines what programmes are feasible. Furthermore, which physical gates are available also matters, as transpiling to native gates will increase the circuit depth. Lastly, the degree of gate parallelisation can allow for shallower circuits and increased performance.

% With all these factors in mind, the metric of quantum volume is defined, giving a single number describing the performance. It is effectively defined as the largest rectangular circuit of two-qubits a quantum computer may execute.

% \subsection{Torlai et al. (2020)}
% Due to the probabilistic nature of quantum computers and their exponentially great number of states, measuring complex observables accurately requires  many samples. By post-processing the measurements using an artificial neural network, the variance of the samples are significantly reduced, though at the cost of some increased bias.

% \subsection{Schuld et al. (2019)}
% In optimising the parameters of variational circuits, having access to the gradient  of the cost function (with respect to the parameters) is beneficial. The individual measurements are probabilistic, but the expectation is a deterministic value whose gradient can be calculated. Often, this is possible exactly using the parameter shift rule, allowing for evaluating the gradient using the same circuit with changed parameters. For circuits containing gates whose derivatives are not as nice, a method of linear combination of unities can be used. This method requires an extended circuit including an ancillary qubit.

% \subsection{Pesah et al. (2021)}
% The problem of barren plateaus plagues the optimisation of variational circuits and quantum neural network; for randomly initialised ans√§tze, the gradient of the cost function may exhibit exponentially small gradients, prohibiting gradient based optimisation. Under certain assumptions, it is shown that for quantum convolutional neural networks, the gradient of the cost function is no worse than polynomially small, such that the networks can be trainable.

% \subsection{Farhi et al. (2018)}
% Quantum neural networks (QNNs) are simply an abstraction of parametrised quantum circuits with some sort of data encoding. As with classical neural networks or supervised learning in general, the parameters are optimised by minimising a cost function. For QNNs, the output can be a single designated read-out qubit, where the states are interpreted as classes in a binary classification problem. This was shown to indeed be feasible for handwritten digit recognition, using downsampled MNIST data. With the qubit count on current quantum devices and the amount that can be easily simulated, the dimensionality of the data can not be much more than a dozen.


% \subsection{Abbas et al. (2021)}
% Whether quantum neural networks have inherent advantages is still an open question. Using the Fisher information of models, the authors calculate the effective dimension as a measure of expressibility. For models comparable in their input, output and parameter count, the effective dimension of particular quantum neural networks can be significantly higher. This advantage is empirically shown to be useful with a particular model on real quantum hardware, showing convergence in fewer steps than a similarly specced classical network.

% The importance of feature maps is remarked upon, affecting both the expressibility of the model and the risk of barren plateaus, which in turn determines trainability.
